{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90165498-d27c-467c-918d-9add53be23bc",
   "metadata": {},
   "source": [
    "## Step 1: Data Splitting\r\n",
    "\r\n",
    "### What We're Doing:\r\n",
    "1. **Separate Features and Target**: \r\n",
    "   - We take our dataset (a DataFrame) and separate the features (all columns except 'Attack Type') and the target (the 'Attack Type' column).\r\n",
    "\r\n",
    "2. **Split Data**: \r\n",
    "   - We then split the data into training and testing sets. \r\n",
    "   - The training set is used to train the model, and the testing set is used to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf3c505d-74e8-4e71-9343-74ce354ea288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "   Protocol  Packet Length  Packet Type  Traffic Type  Malware Indicators  \\\n",
      "0         0      -0.669295            1             2                   0   \n",
      "1         0       0.943535            1             2                   0   \n",
      "2         2      -1.142808            0             2                   0   \n",
      "3         2      -0.952922            1             2                   1   \n",
      "4         1       1.635778            1             0                   1   \n",
      "\n",
      "   Anomaly Scores  Alerts/Warnings  Attack Type  Attack Signature  \\\n",
      "0       -0.743191                1            2                 1   \n",
      "1        0.048054                1            2                 0   \n",
      "2        1.292975                0            0                 1   \n",
      "3       -1.189588                0            2                 1   \n",
      "4       -1.718818                0            0                 1   \n",
      "\n",
      "   Action Taken  ...  Packet Length Category  Anomaly Category  Used Proxy  \\\n",
      "0             2  ...                       2                 1           1   \n",
      "1             0  ...                       0                 0           0   \n",
      "2             1  ...                       2                 0           1   \n",
      "3             0  ...                       2                 1           0   \n",
      "4             0  ...                       0                 1           1   \n",
      "\n",
      "   Agent Family  Browser Name  Operating System  Device Type  \\\n",
      "0             0             2                16            0   \n",
      "1             0             2                16            0   \n",
      "2             0             2                16            0   \n",
      "3             0             1                12            0   \n",
      "4             0             2                16            0   \n",
      "\n",
      "   Rendering Engine  Destination_IP_malicious  City_Attack_Count  \n",
      "0                 3                         1          -1.025715  \n",
      "1                 3                         4          -0.066897  \n",
      "2                 3                         0          -0.450424  \n",
      "3                 1                         4           0.508394  \n",
      "4                 3                         4          -0.578267  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "\n",
      "Data Splitting Results:\n",
      "Training set shape (features): (32000, 41)\n",
      "Training set shape (target): (32000,)\n",
      "Testing set shape (features): (8000, 41)\n",
      "Testing set shape (target): (8000,)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset (ensure you replace 'your_dataset.csv' with your actual file)\n",
    "# For example, if your dataset is in CSV format:\n",
    "df = pd.read_csv('Encoded_Cybersecurity_Data.csv')\n",
    "\n",
    "# Drop 'User Information' column from the dataset\n",
    "df = df.drop(columns=['User Information'], errors='ignore')\n",
    "\n",
    "# Display the first few rows of the DataFrame to understand its structure\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Separate the features and the target variable.\n",
    "# 'Attack Type' is our target, so we drop it from features.\n",
    "X = df.drop('Attack Type', axis=1)  # Features: all columns except 'Attack Type'\n",
    "y = df['Attack Type']               # Target: the 'Attack Type' column\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "# test_size=0.3 means 30% of the data is used for testing.\n",
    "# random_state ensures the split is reproducible.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets to confirm the split.\n",
    "print(\"\\nData Splitting Results:\")\n",
    "print(\"Training set shape (features):\", X_train.shape)\n",
    "print(\"Training set shape (target):\", y_train.shape)\n",
    "print(\"Testing set shape (features):\", X_test.shape)\n",
    "print(\"Testing set shape (target):\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022aeaaa-5631-4c11-bdbc-9046290fcd18",
   "metadata": {},
   "source": [
    "## Step 2: Trying Multiple Classification Models and Comparing Them\r\n",
    "\r\n",
    "### What We're Doing:\r\n",
    "1. **Goal**: \r\n",
    "   - Evaluate and compare several classification models to predict the Attack Type.\r\n",
    "\r\n",
    "2. **Approach**: \r\n",
    "   - We'll use a few common classifiers:\r\n",
    "     - **Logistic Regression**: A linear model for classification.\r\n",
    "     - **Random Forest**: An ensemble method that builds multiple decision trees.\r\n",
    "     - **Decision Tree**: A simple tree-based classifier.\r\n",
    "     - **Support Vector Machine (SVM)**: Effective in high-dimensional spaces.\r\n",
    "     - **K-Nearest Neighbors (KNN)**: A simple instance-based classifier.\r\n",
    "\r\n",
    "3. **Evaluation**: \r\n",
    "   - For each model, we'll train it on the training data, predict on the test data, and then compare performance using metrics such as accuracy and the classification report.roblems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "114f9c6e-6600-4b6d-9def-459527ad352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariel\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.3430\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.40      0.36      2636\n",
      "           1       0.35      0.29      0.32      2721\n",
      "           2       0.34      0.34      0.34      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.34      0.34      0.34      8000\n",
      "weighted avg       0.34      0.34      0.34      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.3321\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.38      0.36      2636\n",
      "           1       0.33      0.30      0.32      2721\n",
      "           2       0.33      0.31      0.32      2643\n",
      "\n",
      "    accuracy                           0.33      8000\n",
      "   macro avg       0.33      0.33      0.33      8000\n",
      "weighted avg       0.33      0.33      0.33      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.3340\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.34      0.33      2636\n",
      "           1       0.34      0.33      0.33      2721\n",
      "           2       0.34      0.34      0.34      2643\n",
      "\n",
      "    accuracy                           0.33      8000\n",
      "   macro avg       0.33      0.33      0.33      8000\n",
      "weighted avg       0.33      0.33      0.33      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.3394\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.68      0.45      2636\n",
      "           1       0.36      0.08      0.13      2721\n",
      "           2       0.35      0.27      0.30      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.35      0.34      0.29      8000\n",
      "weighted avg       0.35      0.34      0.29      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.3351\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.47      0.39      2636\n",
      "           1       0.33      0.33      0.33      2721\n",
      "           2       0.34      0.21      0.26      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.34      0.34      0.33      8000\n",
      "weighted avg       0.34      0.34      0.33      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of Model Performance (Accuracy):\n",
      "Logistic Regression: 0.3430\n",
      "Random Forest: 0.3321\n",
      "Decision Tree: 0.3340\n",
      "Support Vector Machine: 0.3394\n",
      "K-Nearest Neighbors: 0.3351\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for models and evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Define a dictionary of models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Dictionary to store the evaluation results for each model\n",
    "results = {}\n",
    "\n",
    "# Loop through each model: train, predict, and evaluate\n",
    "for model_name, model in models.items():\n",
    "    # Train the model using the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[model_name] = acc  # Store the accuracy score\n",
    "    \n",
    "    # Print the model's performance details\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print a summary of the model performance based on accuracy\n",
    "print(\"Summary of Model Performance (Accuracy):\")\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da8e43-7a51-41ad-a3ae-297155a9b23f",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Tuning and Feature Selection\r\n",
    "\r\n",
    "### Part A: Hyperparameter Tuning\r\n",
    "\r\n",
    "#### What We're Doing:\r\n",
    "1. **Goal**: \r\n",
    "   - Optimize model performance by finding the best hyperparameters for each classifier.\r\n",
    "\r\n",
    "2. **Approach**: \r\n",
    "   - Use `GridSearchCV` to systematically explore a grid of hyperparameter values for:\r\n",
    "     - **Logistic Regression**\r\n",
    "     - **Random Forest**\r\n",
    "     - **Decision Tree**\r\n",
    "     - **K-Nearest Neighbors**\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95f20b47-2463-4d56-b1fa-84bb0b854138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19206c68-aa7b-424c-b791-d83e2ce8f7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning Results:\n",
      "==================================================\n",
      "Model: Logistic Regression\n",
      "Best Parameters: {'C': 0.1, 'penalty': 'l2'}\n",
      "Best Cross-Validation Score: 0.3346\n",
      "--------------------------------------------------\n",
      "Model: Random Forest\n",
      "Best Parameters: {'max_depth': 10, 'n_estimators': 100}\n",
      "Best Cross-Validation Score: 0.3359\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Best Parameters: {'max_depth': None, 'min_samples_split': 5}\n",
      "Best Cross-Validation Score: 0.3390\n",
      "--------------------------------------------------\n",
      "Model: K-Nearest Neighbors\n",
      "Best Parameters: {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best Cross-Validation Score: 0.3329\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Step 1: Hyperparameter Tuning\n",
    "# ---------------------------\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "param_grid_logistic = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2']  # Only l2 penalty is used here\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Initialize models with fixed random_state for reproducibility\n",
    "logistic = LogisticRegression(max_iter=3000, random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Map model names to a tuple of (model, hyperparameter grid)\n",
    "models_param = {\n",
    "    'Logistic Regression': (logistic, param_grid_logistic),\n",
    "    'Random Forest': (rf, param_grid_rf),\n",
    "    'Decision Tree': (dt, param_grid_dt),\n",
    "    'K-Nearest Neighbors': (knn, param_grid_knn)\n",
    "}\n",
    "\n",
    "# Dictionary to store the best tuned model for each classifier\n",
    "tuned_models = {}\n",
    "\n",
    "print(\"Hyperparameter Tuning Results:\\n\" + \"=\"*50)\n",
    "# Loop through each model and perform grid search with 5-fold cross-validation\n",
    "for name, (model, param_grid) in models_param.items():\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    tuned_models[name] = best_model\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1675763f-bcfe-4074-a696-428303724a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances from Tuned Random Forest:\n",
      "                          Feature  Importance\n",
      "5                  Anomaly Scores    0.085125\n",
      "1                   Packet Length    0.084480\n",
      "17                           City    0.078439\n",
      "40              City_Attack_Count    0.065120\n",
      "21                            day    0.059837\n",
      "18                          State    0.058750\n",
      "22                           Hour    0.054058\n",
      "20                          month    0.042975\n",
      "25                    day_of_week    0.035730\n",
      "36               Operating System    0.032609\n",
      "19                           year    0.023963\n",
      "35                   Browser Name    0.020172\n",
      "0                        Protocol    0.019855\n",
      "24                         Season    0.019736\n",
      "3                    Traffic Type    0.019507\n",
      "8                    Action Taken    0.019468\n",
      "39       Destination_IP_malicious    0.018600\n",
      "10                Network Segment    0.018331\n",
      "9                  Severity Level    0.018232\n",
      "38               Rendering Engine    0.016835\n",
      "23                    Time of Day    0.016827\n",
      "31         Packet Length Category    0.013599\n",
      "32               Anomaly Category    0.012676\n",
      "2                     Packet Type    0.011523\n",
      "11                  Firewall Logs    0.011087\n",
      "7                Attack Signature    0.010981\n",
      "4              Malware Indicators    0.010878\n",
      "13                     Log Source    0.010664\n",
      "29           Source Port Category    0.010304\n",
      "30      Destination Port Category    0.010295\n",
      "6                 Alerts/Warnings    0.010210\n",
      "12                 IDS/IPS Alerts    0.010092\n",
      "15  Suspicious Destination Subnet    0.009411\n",
      "14       Suspicious Source Subnet    0.008989\n",
      "26                     is_weekend    0.008539\n",
      "33                     Used Proxy    0.008107\n",
      "16        Suspicious Proxy Subnet    0.008022\n",
      "27              is_business_hours    0.007537\n",
      "37                    Device Type    0.007406\n",
      "34                   Agent Family    0.007039\n",
      "28                  is_lunch_hour    0.003993\n",
      "\n",
      "Selected Features (Importance >= median):\n",
      "['Anomaly Scores', 'Packet Length', 'City', 'City_Attack_Count', 'day', 'State', 'Hour', 'month', 'day_of_week', 'Operating System', 'year', 'Browser Name', 'Protocol', 'Season', 'Traffic Type', 'Action Taken', 'Destination_IP_malicious', 'Network Segment', 'Severity Level', 'Rendering Engine', 'Time of Day']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Step 2: Feature Scoring & Selection\n",
    "# ---------------------------\n",
    "\n",
    "# Use the tuned Random Forest model to extract feature importances\n",
    "best_rf = tuned_models['Random Forest']\n",
    "\n",
    "# Refit the best Random Forest model on the full training data (if needed)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importances and create a DataFrame for easy viewing\n",
    "feature_importances = best_rf.feature_importances_\n",
    "feature_names = X_train.columns  # Assumes X_train is a DataFrame\n",
    "\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances from Tuned Random Forest:\")\n",
    "print(importances_df)\n",
    "\n",
    "# Select features with importance above or equal to the median importance\n",
    "threshold = np.median(feature_importances)\n",
    "selected_features = importances_df[importances_df['Importance'] >= threshold]['Feature'].tolist()\n",
    "\n",
    "print(\"\\nSelected Features (Importance >= median):\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7157efb-051f-4ac7-8cb5-e7ee2e91ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Tuned Models on Test Data (All Features):\n",
      "==================================================\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.3436\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.40      0.37      2636\n",
      "           1       0.35      0.29      0.32      2721\n",
      "           2       0.34      0.34      0.34      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.34      0.34      0.34      8000\n",
      "weighted avg       0.34      0.34      0.34      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.3395\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.42      0.37      2636\n",
      "           1       0.35      0.26      0.30      2721\n",
      "           2       0.34      0.34      0.34      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.34      0.34      0.34      8000\n",
      "weighted avg       0.34      0.34      0.34      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.3354\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.35      0.34      2636\n",
      "           1       0.35      0.33      0.34      2721\n",
      "           2       0.34      0.32      0.33      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.34      0.34      0.34      8000\n",
      "weighted avg       0.34      0.34      0.34      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.3356\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.49      0.39      2636\n",
      "           1       0.34      0.26      0.30      2721\n",
      "           2       0.34      0.26      0.30      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.34      0.34      0.33      8000\n",
      "weighted avg       0.34      0.34      0.33      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of Tuned Model Performance (All Features):\n",
      "Logistic Regression: 0.3436\n",
      "Random Forest: 0.3395\n",
      "Decision Tree: 0.3354\n",
      "K-Nearest Neighbors: 0.3356\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Step 3: Comparison Using All Features\n",
    "# ---------------------------\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "results_tuned_all = {}\n",
    "print(\"\\nEvaluating Tuned Models on Test Data (All Features):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results_tuned_all[name] = acc\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Summary of Tuned Model Performance (All Features):\")\n",
    "for name, accuracy in results_tuned_all.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bb1cc63-f878-4211-b13a-d67f59e039b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Tuned Models on Test Data (Selected Features):\n",
      "==================================================\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.3416\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.48      0.40      2636\n",
      "           1       0.35      0.26      0.30      2721\n",
      "           2       0.34      0.29      0.31      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.34      0.34      0.34      8000\n",
      "weighted avg       0.34      0.34      0.34      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.3349\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.42      0.37      2636\n",
      "           1       0.34      0.25      0.29      2721\n",
      "           2       0.34      0.34      0.34      2643\n",
      "\n",
      "    accuracy                           0.33      8000\n",
      "   macro avg       0.34      0.34      0.33      8000\n",
      "weighted avg       0.34      0.33      0.33      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.3262\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.35      0.33      2636\n",
      "           1       0.34      0.32      0.33      2721\n",
      "           2       0.32      0.31      0.32      2643\n",
      "\n",
      "    accuracy                           0.33      8000\n",
      "   macro avg       0.33      0.33      0.33      8000\n",
      "weighted avg       0.33      0.33      0.33      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.3372\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.48      0.39      2636\n",
      "           1       0.35      0.27      0.30      2721\n",
      "           2       0.33      0.26      0.29      2643\n",
      "\n",
      "    accuracy                           0.34      8000\n",
      "   macro avg       0.34      0.34      0.33      8000\n",
      "weighted avg       0.34      0.34      0.33      8000\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of Tuned Model Performance (Selected Features):\n",
      "Logistic Regression: 0.3416\n",
      "Random Forest: 0.3349\n",
      "Decision Tree: 0.3262\n",
      "K-Nearest Neighbors: 0.3372\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Step 4: Comparison Using Selected Features\n",
    "# ---------------------------\n",
    "\n",
    "# Create training and testing sets with only the selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "results_tuned_selected = {}\n",
    "print(\"\\nEvaluating Tuned Models on Test Data (Selected Features):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    # Retrain the model on the training data with selected features\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred_sel = model.predict(X_test_selected)\n",
    "    acc_sel = accuracy_score(y_test, y_pred_sel)\n",
    "    results_tuned_selected[name] = acc_sel\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Accuracy: {acc_sel:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_sel))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Summary of Tuned Model Performance (Selected Features):\")\n",
    "for name, accuracy in results_tuned_selected.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339765a-20ff-4cf3-8ce9-73070d346376",
   "metadata": {},
   "source": [
    "## XGBoost Classification with Stratified Train-Validation Split\r\n",
    "\r\n",
    "### Introduction:\r\n",
    "This section demonstrates an end-to-end pipeline for training and evaluating an XGBoost classifier using a stratified train-validation split. By ensuring that the class distribution in the training and validation sets mirrors that of the original data, we achieve a balanced evaluation of the model. The process involves verifying the dataset, performing a stratified split, training the model with fixed hyperparameters, and then evaluating its performance using accuracy, log loss, and a detailed classification report.ustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad4b2d1a-7063-4581-aa81-408fb434dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1 2]\n",
      "\n",
      "Training set shape: (25600, 41)\n",
      "Validation set shape: (6400, 41)\n",
      "\n",
      "Validation Accuracy: 0.3362\n",
      "Validation Log Loss: 1.1111\n",
      "\n",
      "Classification Report on Validation Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.36      0.35      2158\n",
      "           1       0.34      0.32      0.33      2109\n",
      "           2       0.33      0.34      0.33      2133\n",
      "\n",
      "    accuracy                           0.34      6400\n",
      "   macro avg       0.34      0.34      0.34      6400\n",
      "weighted avg       0.34      0.34      0.34      6400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Dataset Verification\n",
    "# =============================================================================\n",
    "\n",
    "# Ensure that X_train and y_train are defined.\n",
    "if 'X_train' not in globals() or 'y_train' not in globals():\n",
    "    raise ValueError(\"X_train and y_train must be defined. Please load your dataset and split it accordingly.\")\n",
    "\n",
    "# Display unique classes in y_train to verify distribution.\n",
    "unique_classes = np.sort(np.unique(y_train))\n",
    "print(\"Unique classes in y_train:\", unique_classes)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Stratified Train-Validation Split\n",
    "# =============================================================================\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split) with stratification.\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "print(\"\\nTraining set shape:\", X_tr.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 3: Model Training with XGBoost\n",
    "# =============================================================================\n",
    "\n",
    "# Define fixed hyperparameters for the XGBoost classifier.\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'logloss',\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Initialize and train the XGBoost classifier on the training subset.\n",
    "model = XGBClassifier(**xgb_params)\n",
    "model.fit(X_tr, y_tr)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 4: Evaluation on the Validation Set\n",
    "# =============================================================================\n",
    "\n",
    "# Make predictions on the validation set.\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_proba = model.predict_proba(X_val)  # Needed for log loss calculation.\n",
    "\n",
    "# Compute evaluation metrics.\n",
    "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "loss_val = log_loss(y_val, y_val_proba)\n",
    "report_val = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Display evaluation results.\n",
    "print(\"\\nValidation Accuracy: {:.4f}\".format(accuracy_val))\n",
    "print(\"Validation Log Loss: {:.4f}\".format(loss_val))\n",
    "print(\"\\nClassification Report on Validation Set:\\n\", report_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d73495-603e-4332-a9ad-a7bd9eb2c670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
